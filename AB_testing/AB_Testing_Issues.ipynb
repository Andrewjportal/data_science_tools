{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical issues with A/B testing\n",
    "\n",
    "Let's continue our example. If we have a \"bank\" of customers, we could fire off 3826 emails of variation A, 3826 emails of variation B, and then if B performed better, we could email the rest of our customers variation B. If A did better, or there was no clear winner, we would fall back to the default of A.\n",
    "\n",
    "Let's say that we were interested in only targeting new signups, and that our site had 3000 people join a week. Then how long would we run our test?\n",
    "\n",
    "A naive estimate would be that we need a total of 3826 * 2 = 7652 people. We have 3000 / 7 = 428 people per day. So maybe we should run the experiment for 7652 / 428 = 17 days?\n",
    "\n",
    "This is fine, provided the type of people that visit our site on Monday are the same as the people that visit on Saturday. Often, websites have different volumes of traffic depending on day of the week. Employed people have different representation during the week than after it (over- or under-representation depends on the site: EW.com has a different profile than LinkedIn)\n",
    "\n",
    "If there is a weekly cycle, you are better to round up to the nearest whole week. In this case: 7652 / 3000 = 2.55, so I would run for three weeks before drawing a conclusion. If I was looking at high cost purchases, I might see monthly patterns in my sales (as most people are paid monthly). If I did see significant monthly variation, I would push to extend the experiment to run for a month.\n",
    "\n",
    "You should also consider (at design time) splitting your customers into clusters. Just because variationA outperforms variationB over the entire population, you can find that variationB outperfoms variationA on some subset (e.g. people on the west coast, people that log on after 5 pm, et cetra). You can choose to show different variations for different clusters. If you do this, you should know that the power calculations for sample size are the number of people that need to see each variation _per cluster_, rather than overall.\n",
    "\n",
    "We have phrased A/B testing in terms of clicking through an email. Generally there are two types of A/B tests I have seen:\n",
    "1. Engagement: do people who see variation A use the site more than people who see variation B? We are interested in optimizing the return rate.\n",
    "2. Call-to-action: Each view of a variation asks a user to do something (e.g. open the email). We are interested in optimizing the success rate, which is between 0 and 1.\n",
    "\n",
    "Our methods have all been based around \"call-to-action\" (see the email, then open it). Sometimes it is less clear cut, especially for \"Engagement\" campaigns. If you change the Duolingo Owl graphics to see if you can entice people back to learn a language more, are you going to count the number of visits that user has to the site for the next day as correlated with the campaign? The next week? The next month? Determining the answer to these questions is often a mix of looking at historical records of your users, and experimentation.\n",
    "\n",
    "Another common use-case is looking at user-flow through a sign-up process. When do the users leave? If you show them a different process, do more of them sign up? Are you optimizing for total number of signups, or are you interested in only particular demographics. To avoid multiple testing, you should plan you experiment (including the metrics) **before** running the experiment.\n",
    "\n",
    "A case that has less to do with web-development: you can also A/B test your pricing and discount structures. Uber and Lyft do this when experimenting with surge pricing and incentives for drivers. Here you have to be particularly careful, because the experiments can interfere with each other. A great example of this is on Lyft's blog:\n",
    "\n",
    "https://eng.lyft.com/experimentation-in-a-ridesharing-marketplace-b39db027a66e\n",
    "\n",
    "The experiment here was to compare a passengers that didn't get charged a surge with those that do. In an experiment where there is only one car nearby to pickup, the person with the cheaper fare will be more likely to request a ride than the person with the more expensive fare. This is what Lyft want to measure. But the effect is compounded by \"blocking\": whoever takes the car first stops the other person from taking the same car. In a naive A/B test or simulation where you randomly assign people to the variations, the people with the discount are expected to take $3\\times$ more rides compared to those that don't. If you design the experiment differently to eliminate \"blocking\" (e.g. discount surges for everyone at randomly selected times) the actual effect is closer to a $1.33$ multiplier -- a huge difference!\n",
    "\n",
    "\n",
    "You should aslo communicate to the devs on your team that, where possible, the same user should see the same variation each time. If Amazon is running an A/B test with green \"buy\" buttons, they should make sure that I see a green \"buy\" button until the experiment is over. Randomly seeing different elements can make people skeptical about using a site.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What not to do: early stopping\n",
    "\n",
    "It might be tempting to skip all the power calculations and just work \"until you have reached a p-value of 0.05\" and then take the winner.\n",
    "\n",
    "**This is bad**\n",
    "\n",
    "The p-value assumes that you do the check _once_. As a rough guide, if you do $N$ checks, if there is no difference, each time you have a probability of $\\alpha$ of (falsely) claiming a winner. The probability of (correctly) claiming at no one winners in N trials is roughly\n",
    "$$(1-\\alpha)^N$$\n",
    "I say roughly, because the trails are not independent of each other. But assuming this formula is correct, here is a table showing the effect of multiple evaluations for $\\alpha = 0.05$\n",
    "\n",
    "| Number of evaluations | Prob of correctly saying no difference | Prob of concluding difference |\n",
    "|---:|---:|---:|\n",
    "| 1 | 0.95 | 0.05 |\n",
    "| 2 | 0.90 | 0.10 |\n",
    "| 5 | 0.77 | 0.23 |\n",
    "|10 | 0.59 | 0.41 |\n",
    "\n",
    "Let's run a simulation, where at the end of the day we do a significance test until we reach the end. We will assume 200 visits a day (100 for each variation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the functions we defined in previous notebook, which have\n",
    "# been placed in a separate file\n",
    "from abTests import variation, draw_clicks_from_n_samples, draw_p_sample_from_n_samples\n",
    "from scipy.stats import norm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_day(pA, pB, num_visits=100):\n",
    "    cA = draw_clicks_from_n_samples(num_visits, pA, 1)[0]\n",
    "    cB = draw_clicks_from_n_samples(num_visits, pB, 1)[0]\n",
    "    return (variation(clicks=cA, impressions=num_visits),\n",
    "            variation(clicks=cB, impressions=num_visits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_variation(var_before, var_new):\n",
    "    return variation(clicks=var_before.clicks + var_new.clicks,\n",
    "                     impressions=var_before.impressions + var_new.impressions)\n",
    "\n",
    "\n",
    "def get_prop_from_variation(var):\n",
    "    return var.clicks / var.impressions\n",
    "\n",
    "\n",
    "def get_p_value_analytic(variationA, variationB):\n",
    "    p_A, p_B = (variationA.clicks/variationA.impressions,\n",
    "                variationB.clicks/variationB.impressions)\n",
    "    p = (variationA.clicks + variationB.clicks)/(variationA.impressions + variationB.impressions)\n",
    "    variance = p*(1-p)/variationA.impressions + p*(1-p)/variationB.impressions\n",
    "\n",
    "    abs_z = abs(p_A - p_B)/np.sqrt(variance)\n",
    "\n",
    "    p_value = 2*(1-norm.cdf(abs_z))\n",
    "    return p_value\n",
    "\n",
    "\n",
    "def run_test_with_peeking(n_days, pA, pB, alpha=0.05):\n",
    "    cumulative_var_A = variation(clicks=0, impressions=0)\n",
    "    cumulative_var_B = variation(clicks=0, impressions=0)\n",
    "    for day in range(n_days):\n",
    "        varA, varB = do_day(pA, pB)\n",
    "        cumulative_var_A = cumulative_variation(cumulative_var_A, varA)\n",
    "        cumulative_var_B = cumulative_variation(cumulative_var_B, varB)\n",
    "        p_value = get_p_value_analytic(cumulative_var_A, cumulative_var_B)\n",
    "        if p_value < alpha:\n",
    "            if get_prop_from_variation(cumulative_var_A) > get_prop_from_variation(cumulative_var_B):\n",
    "                winner = 'A'\n",
    "            else:\n",
    "                winner = 'B'\n",
    "            return {\n",
    "                'day': day,\n",
    "                'winner': winner,\n",
    "                'p': p_value\n",
    "            }\n",
    "    return {\n",
    "        'day': day,\n",
    "        'winner': None,\n",
    "        'p': p_value\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a single time with pA = pB = 5% over 30 days\n",
    "run_test_with_peeking(30, 0.05, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's simulate 1000 experiments. How often are they called?\n",
    "all_exp = [run_test_with_peeking(30, 0.05, 0.05) for _ in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many had the correct answer, None, as the winner?\n",
    "num_type_1_errors = len([e for e in all_exp if e['winner'] is not None])\n",
    "frac_type_1_errors = num_type_1_errors/len(all_exp)\n",
    "print(\"With peeking and alpha=0.05, we had a type 1 error rate of {}\".format(frac_type_1_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# How long did it take to reach the incorrect conclusion?\n",
    "num_days = [e['day'] for e in all_exp if e['winner'] is not None]\n",
    "plt.hist(num_days, bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative to early stopping: multi-armed bandit\n",
    "\n",
    "The multi-armed bandit approach to the same problem is modelling off the following: each variation is like a slot machine. You have a certain number of customers to \"spend\", and you might get a reward if they win. You have to decide how to \"spend your customers\" (i.e. how to split them amongst the different variations)\n",
    "\n",
    "The basic idea is simple:\n",
    "1. Play a \"few\" rounds, and see how well each variation does (exploratory phase)\n",
    "2. When a user arrives, assign her randomly to a variation. Variations that are doing better (i.e. have higher conversion rates) have a higher probability of being picked.  Record what the new user does (success vs fail). Update the probabilities to reflect current conversion rates. (exploitation phase).\n",
    "\n",
    "The idea is that variations that do well will be selected more often, and variations that do poorly will \"die out\". There are a couple of different ways of implementing step 2 (mapping from success rates to chance of picking a variation).\n",
    "\n",
    "Two common methods are __epsilon greedy__ and __thompson sampling__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon greedy (technically epsilon-first greedy)\n",
    "\n",
    "Basic setup:\n",
    "\n",
    "You will need to pick $N$ (the number of users you use to evaluate the variations initially), and $\\epsilon$ (the probability of exploring).\n",
    "\n",
    "1. Use the first N users (you choose N) to determine the success rate of each variation\n",
    "2. For each user, pick a number between 0 and 1. If the number is less than $\\epsilon$, randomly choose a variation. Otherwise, just send the user to the current best variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_success(prob):\n",
    "    return np.random.rand() < prob\n",
    "\n",
    "\n",
    "def epsilon_greedy_sim(N0, epsilon, actual_probs, num_trials):\n",
    "    \"\"\"\n",
    "    N0: integer, number of users to send to each variation\n",
    "    epsilon: float between 0 and 1. Probability of exploring new variation\n",
    "    actual_probs: a list of probabilities of success for each variation\n",
    "    num_trials: num of users total to use after the initialization\n",
    "\n",
    "    returns a list of success rates found by the simulation\n",
    "    \"\"\"\n",
    "    current = [variation(clicks=sum([is_success(p) for _ in range(N0)]), impressions=N0)\n",
    "                     for p in actual_probs]\n",
    "    for _ in range(num_trials):\n",
    "        if is_success(epsilon):\n",
    "            # randomly pick a variation\n",
    "            var_to_do = np.random.randint(0, len(actual_probs) - 1)\n",
    "        else:\n",
    "            var_to_do = np.argmax(np.array([get_prop_from_variation(v) for v in current]))\n",
    "        current[var_to_do] = cumulative_variation(current[var_to_do],\n",
    "                                                  variation(clicks=is_success(actual_probs[var_to_do]), impressions=1))\n",
    "\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is an example using 3 variations with\n",
    "# 4% conversion rate, 3% and 2% respectively\n",
    "np.random.seed(42)\n",
    "epsilon_greedy_sim(100, 0.1, [0.04, 0.03, 0.02], 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this identified mostly variation A, which is what we want. If the variations are close, a sub-par variation can take the lead early, and then keeps it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(60)\n",
    "epsilon_greedy_sim(100, 0.1, [0.04, 0.03, 0.02], 10000)  # burn a random seed\n",
    "epsilon_greedy_sim(100, 0.1, [0.04, 0.03, 0.02], 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ways to avoid this:\n",
    "- Increase $N0$: require more sampling initially, so it is harder for a bad variation to get a head start\n",
    "- Increase $\\epsilon$: Make more random pulls. This gives good variations more of a chance to catch up\n",
    "\n",
    "Note that both of these techniques require me to give up making the current best choice. The big difference is $\\epsilon$ is \"forever\" -- if I have $\\epsilon=0.15$ I am deciding that I am always experimenting with 15% of my data. \n",
    "\n",
    "If $\\epsilon$ is small, we cannot catch up from a suboptimal variation that started with a string of good luck.\n",
    "\n",
    "If two variations have similar conversion rates, then it is easier for the suboptimal ones to \"win\". The motivation of using the bandit algorithms is you don't \"waste\" as much time finding the best variation, and even if you pick a suboptimal variation it is likely to be a better suboptimal variation.\n",
    "\n",
    "#### Pitfall\n",
    "\n",
    "If you have weekly variation in your data, you don't want to make your initial data collection less than a week. If a traditional A/B test would work in that time frame, then you should do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thompson sampling\n",
    "\n",
    "Basic idea:\n",
    "1. Start every variation as having a success probability drawn from p_i ~ Beta(1,1) i.e. uniform\n",
    "2. When a new user comes in, draw a random probability of success from each Beta(S_i+1, F_i+1) distribution, where $S_i$ is the number of successes in variation $i$, and $F_i$ is the number of failures in variation $i$.\n",
    "3. Assign the user to whichever variation had the largest probability drawn in step 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "\n",
    "beta.rvs(a=1, b=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thompson_sim(actual_probs, num_trials):\n",
    "    \"\"\"\n",
    "    actual_probs: a list of probabilities of success for each variation\n",
    "    num_trials: num of users total to use after the initialization\n",
    "\n",
    "    returns a list of variations about what happened in the experiments\n",
    "    \"\"\"\n",
    "    current = [(1, 1) for _ in actual_probs]\n",
    "    for _ in range(num_trials):\n",
    "        prob_draw = np.array([beta.rvs(Alpha, Beta) for Alpha, Beta in current])\n",
    "        to_use = np.argmax(prob_draw)\n",
    "        trial_success = is_success(actual_probs[to_use])\n",
    "        current[to_use] = (current[to_use][0] + trial_success, current[to_use][1] + (1-trial_success))\n",
    "\n",
    "    return [variation(clicks=Alpha, impressions=Alpha + Beta) for Alpha, Beta in current]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "thompson_sim([0.04, 0.03, 0.02], 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros and cons\n",
    "\n",
    "| Property | A/B testing (p-value)| MAB: Epsilon | MAB: Thompson |\n",
    "|---| --- | --- | --- |\n",
    "| Fixed time | Yes | No | No|\n",
    "| Continual costs | No (experiment ends) | Yes | Yes |\n",
    "| Parameters | $\\alpha$, power, time | $N_0$, $\\epsilon$, time| time |\n",
    "| Early Leaders bias results | No | Yes, strongly | Yes, but only mild |\n",
    "| Adaptive | No | Yes | Yes |\n",
    "| _Natural_ generalizes to many variations | No(\\*) | Yes | Yes |\n",
    "| Makes Type I guarantees | Yes | No | No |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "* **DO** a power calculation ahead of time. Make sure you have some idea of what the experiment \"costs\" to run, and how certain you need to be.\n",
    "* **DO** an analysis of whether a standard A/B test makes sense, or if you should use a MAB strategy. \n",
    "    * Standard A/B tests are usually for cases where you have an established standard or you are more risk adverse. \n",
    "    * MAB is useful for getting through many iterations quickly, and the cost of making a suboptimal decision isn't critial.\n",
    "    * See table above for clarity\n",
    "* **DON'T** stop mid-cycle. Identify significant periods for your problem, and do a whole number of these cycles.\n",
    "* **DO** break customers into segments as appropriate.\n",
    "* **DON'T** rely on this checklist -- go back and **read** the section _Practical issues with A/B testing_. Experimental design is hard and subtle, and doing the wrong design can get you \"the right answer to the wrong question\" rendering your analysis useless. This is something to take your time on!\n",
    "* **DO** re-read the _Practical issues with A/B testing_ section!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
